{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textgen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_path = textgen.get_resource_path(\"toxicity-data-merged-comments-05251530.csv\")\n",
    "comments = pd.read_csv(comments_path, index_col=0)\n",
    "posts_path = textgen.get_resource_path(\"toxicity-data-merged-posts-05251530.csv\")\n",
    "posts = pd.read_csv(posts_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_wv(fpath):\n",
    "    with open(fpath, \"rb\") as fin:\n",
    "        (itos, stoi, vectors) = pickle.load(fin)\n",
    "        src_vocab = textgen.Vocabulary(itos, stoi, vectors)\n",
    "    return src_vocab\n",
    "src_vocab = load_pretrained_wv(textgen.get_resource_path(\"word_vectors.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [x for x in chain.from_iterable([x.split(\"\\\\\") for x in comments.CommentContent.tolist()])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_vocab = textgen.Vocabulary()\n",
    "tgt_vocab.build_vocabulary(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Vocabulary: 27074 items, emb dim: (27074, 300)>, <Vocabulary: 1468 items>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab, tgt_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\seantyh\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.751 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4477, 309, 2494, 800, 18527, 2350, 49, 458, 163, 190, 5323, 148, 106, 63, 11154, 656, 5786, 931, 476, 39, 139, 737, 110, 11710, 7317, 2494, 3742, 355, 347, 656, 5786, 1336, 18440, 778, 24042, 807, 7949, 826, 5786, 11694, 656, 2900, 5786, 16030, 28, 2941, 3990, 311, 5786, 11694, 2011, 51, 61, 476, 39, 139, 737, 11710, 7317, 6857, 119, 7317, 39, 78, 6857, 7317, 22142, 163, 857, 23029, 4638, 1796, 1314, 93, 6857, 163, 190, 12285, 122, 2494, 183, 2607, 114, 2002, 18527, 19868, 2484, 748, 280, 163, 119, 320, 7576, 2494, 546, 183, 1348, 17673, 1292, 18462, 163, 2464, 1909, 894, 2017, 10072, 320, 974, 757, 101, 163, 320, 276, 61, 1292, 1314, 529, 894, 313, 163, 14, 1314, 19780, 192, 8356, 2874, 1308, 153, 14769, 153, 163, 1314, 239, 119, 320, 6609, 2918, 347, 1292, 352, 671, 857, 1314, 3145, 843, 163, 1314, 19780, 163, 2874, 1308, 153, 857, 14769, 31, 1292, 408, 413, 18527, 270, 6109, 163, 8173, 843, 430, 488, 2494, 546, 183, 8161, 476, 847, 163, 6410, 229, 163, 843, 69, 355, 772, 22649, 163, 2642, 153, 239, 32, 153, 9711, 4, 2464, 2045, 12897, 163, 2642, 153, 843, 69, 1450, 114, 843, 189, 3145, 119, 2642, 3145, 1314, 114, 12159, 3806] [0]\n"
     ]
    }
   ],
   "source": [
    "for ridx, row in comments.iterrows():\n",
    "    try:\n",
    "        txt = posts.loc[row.TextId, :]\n",
    "    except KeyError:\n",
    "        print(f\"Cannot find {row.TextId}\")\n",
    "        continue\n",
    "    text_vec = textgen.convert_text(txt.TextContent, src_vocab)\n",
    "    tgt_vec = textgen.convert_comment(row.CommentContent, tgt_vocab)\n",
    "    print(text_vec, tgt_vec)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
